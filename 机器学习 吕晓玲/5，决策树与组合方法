#决策树和组合方法
#数据简介
##乳腺癌数据（bispsy）是从威斯康星卖迪逊大学医院获取的，有699个肿瘤切片的诊断信息
##数据共有11个变量，有9个为是否为恶性癌症有关
#描述统计
library(MASS)
data("biopsy")
biopsy<- biopsy[-1]#把第一列ID去掉
table(biopsy$class)
library(mice)
md.pattern(biopsy)#生成缺失数据报告
set.seed(1234567)
train<- sample(1:nrow(biopsy),round(0.8*nrow(biopsy)))#抽取80%作为训练集
#分类树
##首先调用tree包中的tree（）函数建立一棵分类决策树
library(tree)
bio.tree<- tree(class~.-class,biopsy[train,])
summary(bio.tree)
bio.tree.pred<- predict(bio.tree,biopsy[-train,],type = 'class')
table(bio.tree.pred,biopsy[-train,'class'])
plot(bio.tree)
text(bio.tree)
#bagging方法
##bagging是随机森林的一种特殊情况，即在每次分割时不对变量做随机抽样
library(randomForest)
bio.bag<- randomForest(class~.,biopsy[train,],na.action=na.roughfix,
                       mtry=ncol(biopsy)-1)
#在上述函数中mtry=ncol(biopsy)-1用来指定每次分割当中用到的变量的个数
#除输出变量外的9个变量，用na.action=na.roughfix来指定用列的中位数代替缺失数据
bio.bag.pred<- predict(bio.bag,biopsy[-train,])
table(bio.bag.pred,biopsy[-train,'class'])
#数据描述
#cpu数据，cpu数据集中有209个cpu的9中性能指标
#描述统计
library(MASS)
data(cpus)
cpus<-cpus[,2:(ncol(cpus)-1)]
cpus<- data.frame(cpus)
windowsFonts(A=windowsFont("Times New Roman"),
             B=windowsFont("Arial Black"))
par(family="A")
pairs(cpus)#绘制散点图
hist(cpus$perf)#使用hist函数做因变量perf的直方图
#由直方图可以看出perf严重右偏考虑取perf的对数使之正太化
hist(log10(cpus$perf))
cpus$perf=log10(cpus$perf)
set.seed(1234)
train=sample(1:nrow(cpus),round(0.8*round(nrow(cpus))))
#回归树的建立
library(tree)
attach(cpus)
cpus_tree=tree(perf~.,cpus[train,])
summary(cpus_tree)
plot(cpus_tree)
text(cpus_tree)
#bagging算法
#与分类问题相似，我们用randomForest()函数构造bagging模型
#指定每棵树都用全部6个自变量，并将模型用于测试集
###构造bagging模型
library(randomForest)
cpus_bag<- randomForest(perf~.,cpus[train,],mtry=ncol(cpus)-1)
###用于测试集
cpus_bag.pred<- predict(cpus_bag,cpus[-train,])
mean((cpus_bag.pred-cpus[-train,'perf'])^2)
#随机森林方法
#再用randomForest函数构造随机森林模型
library(randomForest)
cpus_rf<- randomForest(perf~.,cpus[train,],importance=T)
#对于回归问题，随机森林默认p/3个变量构造一棵树，在本例
#即用2个变量，用importance=T计算变量重要性，构造模型后
#可用importance（）输出各变量的重要性
cpus_rf.pred<- predict(cpus_rf,cpus[-train,])
mean((cpus_rf.pred-cpus[-train,'perf'])^2)
#boosting模型
library(gbm)
cpus_boost<- gbm(perfZ~.,cpu[train,],distribution='gaussian',
                 n.trees=5000,interaction.depth=4)
#以上假设分布为高斯的
cpus_boost.pred<- predict(cpus_boost,cpus[-train,],ntrees=5000)
mean((cpus_boost.pred-cpus[-train,'perf'])^2)
